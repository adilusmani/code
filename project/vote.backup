#from pyspark.sql import SparkSession
#This program retrieves tweets with @HillaryClinton tags, and rates it for positive, negative and neutral categories
#We have to ignore the rating as this is an inefficient approach because we have less training data. We hae to use NLTK
#Or some other libraries for the same, that we can try this semester. The main goal is to retrieve tweets into Spark cluster


import pandas as pd
import numpy as np
import json
import os
from pandas.io.json import json_normalize
from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream
import sys
from collections import Counter
from geopy.geocoders import Nominatim
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import time
import googlemaps


#map = Basemap(projection='robin', resolution = 'l', area_thresh = 1000.0,lat_0=0, lon_0=-130)
#Create a map for US projection
map = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,
            projection='lcc',lat_1=33,lat_2=45,lon_0=-95)

shp_info = map.readshapefile('/Users/dwipam/Downloads/basemap-1.0.7/examples/st99_d00','states',drawbounds=True)

map.drawcoastlines()
#map.drawcountries()
#map.fillcontinents(color = 'gray')
map.bluemarble()
#map.drawmapboundary()
map.drawmeridians(np.arange(0, 360, 30))
map.drawparallels(np.arange(-90, 90, 30))

#Get colors for different statuses
def get_marker_color(stat):
	if stat == 'hate' :
        	return ('ro')
    	elif stat == 'like':
        	return ('yo')
    	else:
        	return ('ro')
#Create a color map on the map that we already created
def color_map(table):
	min_marker_size = 7
	for lon, lat, mag, stat in zip(table['Lon'], table['Lat'], table['prob'],table['status']):
    		x,y = map(lon, lat)
    		msize = mag * min_marker_size
    		marker_string = get_marker_color(stat)
    		map.plot(x, y, marker_string, markersize=msize)
 
	from time import gmtime, strftime
	import pdb;pdb.set_trace()	
	plt.show()

people = []


def handle_twitter():
	#Return data from Twitter
	ACCESS_TOKEN = '759067855123996673-SMh5suAmoGjFjLe9uGnT8kDjBAdygkJ'
	ACCESS_SECRET = 'mXd44Jg5QOkhKmO310ex4Zwabe6wEeApZnC2YEuKdHZVz'
	CONSUMER_KEY = 'pUIwbWWj9nqjQNRU4mioXHnCJ'
	CONSUMER_SECRET = 'ukObCLCVITbL1biri3jheZHsoVeq5iLVplKcsUa1EeczKB8d2G'
	#Read train data with sentiment positive and negative	
	corpus = pd.read_csv('corpus.csv',sep=',',error_bad_lines=False)
	
	negsub = corpus.loc[corpus['Sentiment']==0]
	possub = corpus.loc[corpus['Sentiment']==1]
	g = lambda x : x.split()
	neg = [g(i) for i in negsub['SentimentText']];pos = [g(i) for i in possub['SentimentText']]
        neg = retrieve(neg);pos = retrieve(pos)         
        bad = open("negative-words.txt","r").read().split()
        good = open("positive-words.txt","r").read().split()	
	negcount = Counter(neg);poscount = Counter(pos)
	oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)
	twitter_stream = TwitterStream(auth=oauth)
	twitter = Twitter(auth=oauth)
	x = []
	for i in range(0,1):
		#Read tweets with Hillay Clinton statuses
		iterator = twitter_stream.statuses.filter(track="@HillaryClinton,", language = "en")
		x.append(iterator)
	
	#iterator = twitter.search.tweets(q='HillaryClinton', lang='en', count=10000)
	collectobj = []
	#Create a Data Frame from Json object with coloumns as id, User_name, Tweet_test, Location, probability
	for iterator in x:
		for tweet in iterator:
			if 'user' in tweet.keys() and tweet['user']['location']:
				#Get probability of a tweet being positive, negative or neutral
				prob,topic = getprobtop(tweet['text'],neg,bad,negcount,pos,good,poscount)
				obj = {'id' : tweet['user']['id'], 'User_name' : tweet['user']['screen_name'], \
			'Text': tweet['text'],'location' : tweet['user']['location'],'prob' : prob, \
			'status' : topic}
				collectobj.append(obj)	
	
	table = json_normalize(collectobj)
	return(table)
	

def getprobtop(text,neg,bad,negcount,pos,good,poscount):
	#Get probability of tweet being positive and negative. We can ignore this, and use NLTK to rate
	#Tweet. This is Naive Baye's approach
	text = text.lower()	
	topic_m = []
	for i in text.split():
		p1,t1 = getprob(i,neg,negcount,pos,'hate')
		if(p1 in good):
			p1 = p1*0.8
		p2,t2 = getprob(i,pos,poscount,neg,'like')
		if(p2 in bad):
			p2 = p2*0.8
		p = [p1, p2]; t = [t1, t2];t = zip(p,t);prob, topic = max(item for item in t)
		topic_m.append(topic)
	
	leng = len(topic_m)
	topic = Counter(topic_m)
	key, value = max(topic.iteritems(), key=lambda x:x[1])
	value = value/(leng+0.0)
	
	return(value, key)
			
def plotmap(table):
	#Given a table, this retreives Lattitude and Longitude of the locations
	#gc = Nominatim()
	api_key = 'AIzaSyBECDqcw187wlD_3n8FlJ9MdAvv7bUbKkY'
	gc = googlemaps.Client(api_key)
	location = []
	ids = []
	cities = table['location']
	id = table['id']
	lat = [];lon = []
	import pdb;pdb.set_trace()
	for i in range(0,len(cities)):
		try:
			#import pdb;pdb.set_trace()
			#time.sleep(10)
			loc = gc.geocode(cities[i])[0]['geometry']['location']
		except Exception :
			print(cities[i])
			print Exception
			continue	
		location.append(loc)
		lon.append(loc['lng'])
		lat.append(loc['lat'])
		ids.append(id[i])	
	temp = pd.DataFrame({'id' : ids, 'Lat' : lat, 'Lon' : lon})	
	temp = temp.merge(table,right_on = 'id',left_on = 'id')
	return(temp)

def getprob(word,x,xfreq ,y, topic):
	#Return probability of the word. Please ignore this. This is highly ineffificent as 
	#preprocessed the tweets, such as normalization etc.
	word = word.encode('ascii', 'ignore')
	if word not in x:
		return(0,'neutral')
	probw_1 = xfreq.get(word)/(len(x)+0.0) 
	prob_1 = len(x)/(len(x)+len(y)+0.0)	
	return(probw_1 * prob_1,topic)

def retrieve(arr):
	negs = []	
	for i in arr:
		for x in i:
			negs.append(x)	
	return(negs)
def main():
	#Removing the comments would allow to connect to Spark Master node at 7007 port no. on Local host.
	#So even if program fails, we will be having static data in our Spark file system
	""" spark = SparkSession.builder \
        	.master("spark://localhost-2.local:7077") \
        	.appName("Vote") \
        	.config("spark.some.config.option", "some-value") \
        	.getOrCreate()
	for i in range(1,100):
		print(i)
		q = handle_twitter()
		spark.createDataFrame(q).write.parquet("Votet/key="+str(i)) #Outputs the data to local FS as a parquet file
		time.sleep(15*15)
	"""
	c = pd.DataFrame.from_csv("temp.csv")
	c = plotmap(c)
	color_map(c)
if __name__ == '__main__':
	main()
